{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/restrepo/PythonTipsAndTricks/blob/master/numpy/Append_Dictionary_to_File.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WWvEijTHy0h7"
   },
   "source": [
    "# Append list of Python dictionaries to a file without loading it\n",
    "From: https://stackoverflow.com/a/36246957/2268280\n",
    "\n",
    "If you are looking to not actually **load** the file, going about this with `json` is not really the right approach.  You could use a memory mapped fileâ€¦ and never actually load the file to memory -- a `memmap` array can open the file and build an array \"on-disk\" without loading anything into memory.\n",
    "\n",
    "Create a memory-mapped array of dicts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x8xEJdmEyt5l"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.memmap('mydict.dat', dtype=object, mode='w+', shape=(4,))\n",
    "a[0] = {'name':\"Joe\", 'data':[1,2,3,4]}\n",
    "a[1] = {'name':\"Guido\", 'data':[1,3,3,5]}\n",
    "a[2] = {'name':\"Fernando\", 'data':[4,2,6,9]}\n",
    "a[3] = {'name':\"Jill\", 'data':[9,1,9,0]}\n",
    "a.flush()\n",
    "del a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jo9L7zDjywWf"
   },
   "source": [
    "Now read the array, without loading the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3QOt97QczYm-"
   },
   "outputs": [],
   "source": [
    "a = np.memmap('mydict.dat', dtype=object, mode='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CUnaWhfkzbPu"
   },
   "source": [
    "The contents of the file are loaded into memory when the list is created, but that's not required -- you can work with the array on-disk without loading it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "bdTv-Ps9zpwd",
    "outputId": "bf057491-12a8-4804-b624-721d2ef68351"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Joe', 'data': [1, 2, 3, 4]},\n",
       " {'name': 'Guido', 'data': [1, 3, 3, 5]},\n",
       " {'name': 'Fernando', 'data': [4, 2, 6, 9]},\n",
       " {'name': 'Jill', 'data': [9, 1, 9, 0]}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "undsGo4SzrmC"
   },
   "source": [
    "It takes a negligible amount of time (e.g. nanoseconds) to create a memory-mapped array that can index a file regardless of size (e.g. 100 GB) of the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M12XlV9D1oHj"
   },
   "source": [
    "## Aplications\n",
    "* Filter some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "vJojipVz0qVm",
    "outputId": "00f3d814-8de6-4af5-a658-a3d7b5679ebf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5pOCH5N90zCo"
   },
   "source": [
    "* Load a file into one memmpap:\n",
    "\n",
    "See [loading csv column into numpy memmap (fast)](https://stackoverflow.com/a/36779509/2268280)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cPu8vEfl02UE"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(a.tolist()).to_json(\n",
    "    'kk.json',orient='records',\n",
    "    lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.lib.format import open_memmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/tmp/arr.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-49fef39c3210>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# we need to specify the shape and dtype in advance, but it would be cheap to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# allocate an array with more rows than required since memmap files are sparse.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen_memmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/tmp/arr.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# parse at most 10000 rows at a time, write them to the memmaped array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/data/com.termux/files/usr/lib/python3.7/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mopen_memmap\u001b[0;34m(filename, mode, dtype, shape, fortran_order, version)\u001b[0m\n\u001b[1;32m    807\u001b[0m         )\n\u001b[1;32m    808\u001b[0m         \u001b[0;31m# If we got here, then it should be safe to create the file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m             \u001b[0mused_ver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_write_array_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tmp/arr.npy'"
     ]
    }
   ],
   "source": [
    "# we need to specify the shape and dtype in advance, but it would be cheap to\n",
    "# allocate an array with more rows than required since memmap files are sparse.\n",
    "mmap = open_memmap('/tmp/arr.npy', mode='w+', dtype=np.double, shape=(100000, 2))\n",
    "\n",
    "# parse at most 10000 rows at a time, write them to the memmaped array\n",
    "n = 0\n",
    "for chunk in pd.read_json(\n",
    "    'kk.json',orient='records',\n",
    "    lines=True):#, chunksize=10000):\n",
    "    mmap[n:n+chunk.shape[0]] = chunk.values\n",
    "    n += chunk.shape[0]\n",
    "\n",
    "print(np.allclose(data, mmap))\n",
    "# True"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNDzp7d/tlpRFbpZWUX94Ea",
   "include_colab_link": true,
   "name": "Append_Dictionary_to_File.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
